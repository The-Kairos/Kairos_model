# âœ… **HOW OUR AUDIO PIPELINE WORKS **

*(With Whisper, AST, and Silero VAD explanations)*

Our project extracts two kinds of audio information:

1. **Speech transcription** â†’ using Whisper
2. **Environmental / natural sound detection** â†’ using AST (Audio Spectrogram Transformer)
3. **Speech boundary detection** â†’ using Silero VAD

Everything runs **locally** inside our Python code â€” no API calls.

---

# ğŸ¬ **1. Scene â†’ Audio Extraction (FFmpeg)**

For every video scene, we cut only the audio that belongs to that scene:

```python
extract_scene_audio_ffmpeg(video_path, scene_03.wav, start, end)
```

This creates files like:

```
output/audio/scene_03.wav
```

These become the inputs for ASR + AST.

---

# ğŸ”Š **2. Speech Recognition (ASR) â€” HOW WHISPER ACTUALLY WORKS**

### âœ” We install Whisper locally using:

```bash
pip install openai-whisper
```

### âœ” Whisper GitHub

[https://github.com/openai/whisper](https://github.com/openai/whisper)

### âœ” What Whisper is

Whisper is a deep-learning speech recognition model trained on **680,000 hours** of multilingual audio.
It runs locally on the GPU/CPU. No internet is needed.

### âœ” How we use Whisper

1. We load a Whisper model locally (e.g., `medium`):

```python
model = whisper.load_model("medium")
```

2. We only feed Whisper the **speech-only** audio extracted by VAD.

3. Whisper returns a text transcription.

4. We apply a small filter to remove hallucinated endings like:

   * â€œThank you for watchingâ€
   * â€œThanksâ€
   * â€œThank youâ€

---

# ğŸ” **3. Where Whisper Fits: ASR Pipeline Steps**

### **Step 1 â€” Load audio**

Load as 16kHz mono.

### **Step 2 â€” Noise Reduction**

Using `noisereduce` to remove hiss/hum â†’ helps reduce hallucination.

### **Step 3 â€” Detect speech using VAD (Silero)**

We use **Silero VAD**, a lightweight neural model from GitHub:

### âœ” Silero VAD GitHub

[https://github.com/snakers4/silero-models](https://github.com/snakers4/silero-models)

Silero VAD tells us **where someone is actually speaking**:

```
[
  {"start": 1200, "end": 2400},
  {"start": 5000, "end": 6800}
]
```

These timestamps are in **samples**, not seconds.

### **Step 4 â€” Extract those speech chunks**

We concatenate the speech into one waveform.

### **Step 5 â€” Whisper transcribes**

Whisper produces text.

### **Step 6 â€” Return output**

We save:

```
output/captions/scene_03_asr.txt
```

---

# ğŸŒ³ **4. Natural Sound Detection (AST) â€” HOW AST ACTUALLY WORKS**

We use a HuggingFace model:

### âœ” AST HuggingFace repo

[https://huggingface.co/microsoft/ast](https://huggingface.co/microsoft/ast)

AST = **Audio Spectrogram Transformer**
It is trained on **AudioSet** (2 million sound clips, 527 classes).

### âœ” How AST works

1. Convert audio â†’ log-mel spectrogram
2. Feed spectrogram to transformer
3. Model predicts probabilities for each sound label
4. We keep labels above threshold (e.g., 0.30)

### âœ” What AST detects

527 environmental audio classes, including:

* music
* applause
* crowd noise
* ping
* footsteps
* wind
* traffic
* laughter

AST does **not** detect human speech content â†’ thatâ€™s Whisperâ€™s job.

---

# ğŸ”Š **5. Where AST Fits: AST Pipeline Steps**

### **Step 1 â€” Load audio**

Same as ASR.

### **Step 2 â€” Mask out speech using Silero VAD**

We remove human speech from the audio:

```python
y_masked[start:end] = 0.0
```

This ensures AST focuses on **environmental sounds** only.

### **Step 3 â€” Split audio into 2-second clips**

```
[0â€“2s], [2â€“4s], [4â€“6s], ...
```

Each clip is analyzed separately.

### **Step 4 â€” Extract AST features**

Transform audio â†’ spectrogram â†’ embeddings.

### **Step 5 â€” AST classifies each clip**

We collect:

* detected labels (e.g., "Music", "Applause")
* confidence scores

### **Step 6 â€” Save results**

```
output/audio_labels/scene_03_audio_labels.json
```

---

# ğŸ“„ **6. Example AST Output File (From Our System)**

Your example explained:

```json
[
  {
    "clip_index": 0,
    "start_sec": 0.0,
    "end_sec": 2.0,
    "labels": [],
    "scores": []
  },
  {
    "clip_index": 1,
    "start_sec": 2.0,
    "end_sec": 4.0,
    "labels": ["Music"],
    "scores": [0.5993]
  },
  {
    "clip_index": 2,
    "start_sec": 4.0,
    "end_sec": 6.0,
    "labels": ["Ping"],
    "scores": [0.3767]
  },
  {
    "clip_index": 6,
    "start_sec": 12.0,
    "end_sec": 14.0,
    "labels": ["Applause"],
    "scores": [0.6789]
  },
  {
    "clip_index": 7,
    "start_sec": 14.0,
    "end_sec": 15.8,
    "labels": ["Applause"],
    "scores": [0.7377]
  }
]
```

This means:

* Music happens around 2â€“4 seconds
* A ping sound at 4â€“6 seconds
* Applause around 12â€“16 seconds
* Other segments contain no meaningful environmental sounds

---

# ğŸ§© **7. Final Combined Caption (BLIP + ASR + AST)**

Your final caption is constructed by concatenating:

### **BLIP (visual)**

â€œA video frame of a woman speaking at a podiumâ€¦â€

### **ASR (speech)**

"I'm proudâ€¦ to receive this award."

### **AST (environmental audio)**

Music, Ping, Applause, Applause

### âœ” Final combined caption:

```
BLIP: a video frame of a woman speaking at a podium +
ASR: I'm proud, well in fact I'm very proud, to be the first Pashtun, the first Pakistani, and the youngest person to receive this award. +
AST: Music, Ping, Applause, Applause
```

Every modality contributes:

| Component | Purpose              |
| --------- | -------------------- |
| **BLIP**  | What the camera sees |
| **ASR**   | What humans say      |
| **AST**   | Background sounds    |

Together, they form the **full scene understanding**.
