At 00:00:00.000, The scene appears to be part of an educational or instructional video. The frame displays text with the words "x - cross - validation," suggesting the topic being discussed. A person, referred to as "person #1," is present in the bottom-right corner but remains mostly stationary, possibly a presenter or narrator. The audio introduces the topic by referencing a previous section about validation and its drawbacks, with the phrase, "Welcome back. In the last section we talked about validation and we saw some drawbacks with that method." This scene sets the stage for a discussion on cross-validation.. It says " Welcome back. In the last section we talked about validation and we saw some drawbacks with that method.".At 00:00:06.000, The scene continues as part of an educational or instructional video. The frame shows a man in a white shirt, identified as "person #1," who remains mostly stationary in the bottom-right corner, likely the presenter. The audio introduces the topic of "K-4 Christ validation," which is described as a solution to some previously discussed problems. This appears to be a continuation of the discussion on validation methods, building on the prior explanation of cross-validation.. It says " Now we're going to talk about K-4 Christ validation which will solve some of these problems.".At 00:00:10.400, The scene is part of an educational or instructional video, continuing a discussion on cross-validation techniques. The frame shows a man in a white shirt, referred to as "person #1," who remains mostly stationary in the bottom-right corner, likely the presenter. The audio explains the concept of K-fold cross-validation, describing it as a flexible and powerful method for estimating prediction error and understanding model complexity. The presenter mentions that this technique involves dividing data into K parts, with each part taking turns as the validation set while the others serve as the training set. The phrase, "let me go to the picture here," suggests a transition to a visual aid or diagram to further explain the concept.. It says " This is actually a very important technique that we're going to use throughout the course in various sections and also something that we use in our work all the time. But it's really important to understand K-fold cross-validation. It's used for a lot of methods. It's extremely flexible and powerful technique for estimating prediction error and to give an idea of model complexity. So what's the idea of K-fold cross-validation? Well, it's really in the name. Validation, as we've seen, but done sort of like a K-part play. It's done K times with each part, again, to play the role of the validation set, and the other K-parts playing the role of the training set. So I say here, let me go to the picture here.".At 00:00:54.667, The scene is part of an educational or instructional video on cross-validation techniques. The frame briefly shows a train with a visible number, though its relevance to the topic is unclear. The audio continues the explanation of K-fold cross-validation, with the presenter (likely "person #1") describing the process in detail. They explain dividing a dataset into five parts, using each part as a validation set while the others serve as training sets, and calculating prediction errors across all stages to determine the cross-validation error. The presenter references visual aids, mentioning, "the box looks a bit bigger," likely referring to a diagram or drawing being used to illustrate the concept. This scene builds on prior discussions of validation methods and their applications.. It says " and I'll sort of point the picture as I say it. So here we're doing five full cross-validation. As we'll talk about in more detail, the best choices for K in number of folds is usually about five or 10. Okay, so, and I'll explain, we'll talk about that in a few minutes about why those are good choices. But let's fix here K equals five. So I've taken the dataset, I've divided it random, the samples into five parts, again, of the size of both the same. The box looks a bit bigger, right? Okay, well that's my lack of drawing ability, but anyway, it's supposed to be the same, that's trying to switch the word validation in. So the box is supposed to be about the same size and observation, number of observations. But in this case, the first parts, the validation set, the other four are the training parts. So what we're gonna do, what cross-validation does, it forms this five parts. We're gonna train the model on the four training parts, put together as one big block, take the fitted model and then predict on the validation part and record the error. And then that's phase one. Phase two, we're gonna, the validation set will be part two, this block. All the other four parts will be the training set. We fit the model to the training set and then apply it to this validation part. And in the third stage, this is the validation piece, et cetera. So we have sort of, we have five stages, where each, in each stage, one part gets to play the role of validation set. We have the four parts of the training set. We take all the prediction errors from all five parts, we add them together and that gives us what's called the cross-validation error.".At 00:02:30.933, The current scene is part of an educational or instructional video, continuing a detailed explanation of cross-validation techniques. The frame displays text reading "the details," likely introducing a deeper dive into the topic. The audio features the presenter (likely "person #1") explaining the process of dividing data into K parts, calculating cross-validation error rates, and discussing specific cases like leave-one-out cross-validation. The explanation is technical, focusing on statistical methods and their applications, with references to "mean square error," "validation parts," and "five acts of the play." This scene builds on prior discussions of K-fold cross-validation, providing further clarification and examples.. It says " So now in algebra, I'll basically give you the details of what I said in words. So we'll let the k parts of the data be c1 through ck. So these are the observations that are in each of the five parts. And k was five in our example. And we'll try to make the number of observations about the same in every part. Of course, if n is not a multiple of k of five, we can't do that exactly, but we'll do it approximately. So we'll let n sub k be the number of observations in the kth part. So here's the cross-validation error rate. Basically, this is the mean square error we get by applying the... We fit to the k minus one parts that don't involve part number k. That gives us our fit yi hat for observation i. It's four-fifths of the data in this case. And then we add up the error. This is the mean square error that we obtain now on the validation part using that model. So this is for the kth part. And now we do this for all five parts in turn, the five acts of the play, and then we get the cross-validation error rate. Okay? And a special case of this is leave one out cross-validation, where the number of folds is the same as the number of observations. So that means...".At 00:03:50.533, The current scene appears to be part of an educational or instructional video on cross-validation techniques. The frame briefly displays a train with a visible number, though its relevance to the topic remains unclear. The audio continues the technical explanation, with the presenter (likely "person #1") discussing leave-one-out cross-validation. They describe how each observation is treated as a validation set while the rest form the training set. The audio includes a detailed explanation of the process, referencing statistical methods and validation sets. This scene builds on prior discussions of K-fold cross-validation and its variations, using visual aids and examples to clarify the concepts. The train imagery may serve as a metaphor or visual placeholder, but its connection to the topic is not explicitly explained.. It says " In this picture, there actually would be one box per observation, and in leave one outclass validation, each observation itself gets completed with the validation set, the other N minus one are the training set.".At 00:04:05.867, The current scene appears to be part of an educational or instructional video, continuing a detailed discussion on cross-validation techniques. The frame shows text with a background image of an object, though the specifics of the object are unclear. The audio features a technical explanation of "Leave-One-Out Cross-Validation" (referred to as "Liebland-Cross-Validation"), focusing on its application to least-squares and polynomial models. The presenter, likely "person #1," explains how this method allows for fitting the model on the full dataset without leaving out observations, using the hat matrix to calculate cross-validation sums of squares. The explanation emphasizes the influence of individual observations on their own fit, with the presenter stating, "Hi is the diagonal of the hat matrix... it's a number between zero and one." This scene builds on prior discussions of cross-validation methods, providing further technical details and referencing a book for additional information.. It says " Now, actually, the Liebland-Cross-Validation has a special case of that. It represents a special case in the sense that this cross-validation can be done with what actually happened to refit the model at all. So Liebland-Cross-Validation, at least for a v-squares model or a polynomial model, if the cross-validation, Liebland-Cross-Validation has the following form. So the yi hat is now just a fit on a full dataset. Hi is the diagonal of the hat matrix. So have a look in the book for details, but the hat matrix is the projection matrix that projects y onto the column space of x to give you the fit. This is something that can get computed easily when you fit your v-squares model. So go ahead, Robert. Yeah, we haven't emphasized it, but it's available. It's one of the things that's available when you fit your v-squares model. So the overall point of this is that to do a Liebland-Cross-Validation for these particular models, you don't actually have to leave anything out. You can do the fit on the overall dataset and then extract the information you need to get the cross-validation sum of squares. It's interesting because the hi tells you how much influence an observation has on its own fit. It's a number between zero and one. And so if an observation is very influential in its own fit, you can see it punishes the residual because it divides by a number that's small and it inflates the residual. So it sort of does the right thing here. Okay, so, but...".At 00:05:40.000, The current scene is part of an educational or instructional video discussing advanced statistical methods, specifically cross-validation techniques. The frame displays a computer screen with text reading "a special case," suggesting a focus on a specific variation or application of cross-validation. The audio mentions "Leibniz Cross-Validation" and its computational formula, implying a technical explanation of this method and its advantages. The presenter, likely "person #1," continues to elaborate on statistical concepts, referencing charts and methods discussed in a book. This scene builds on prior discussions of K-fold and Leave-One-Out Cross-Validation, diving deeper into specialized cases and computational approaches.. It says " A better chart, although Leibniz Cross-Validation does have this nice computational formula for most of the methods we talk about in this book.".At 00:05:47.067, The current scene is part of an educational or instructional video on statistical methods, specifically focusing on cross-validation techniques. The frame displays a computer screen with text reading "a special case," likely introducing a specific variation or scenario in cross-validation. The audio features a technical explanation by the presenter (likely "person #1"), discussing why choosing K as 5 or 10 is preferable in K-fold cross-validation compared to leave-one-out cross-validation. The presenter explains that leave-one-out cross-validation results in highly correlated training sets, leading to higher variance in the average error. This scene builds on prior discussions of cross-validation methods, providing further insights into the advantages of specific K values and their impact on statistical learning.. It says " statistical learning methods, it's better to choose K to be 5 or 10 rather than have it have leave an out-crossed validation. And why is that? Well, one problem with leave an out-crossed validation is each of the training sets look very much like the other ones, right? They only differ by one observation. So when you take the average cross-validation is, you take the average of errors over the endfolds and in leave an out-crossed validation, the endfolds look very similar to each other because the training sets are almost the same. They're only different by one observation. So as a result, that average has a high variance because the ingredients are highly correlated. So that's the main reason why it's thought, and we also agree that a better choice for K and cross-validation is the same.".At 00:06:32.000, The current scene is part of an educational or instructional video on statistical methods, continuing a detailed discussion of cross-validation techniques. The frame displays a computer screen with the text "a special case," suggesting a focus on a specific variation or application of cross-validation. The audio is mostly silent, with low-confidence fragments mentioning "5 or 10" and "V1 out cross-validation," likely referring to K-fold cross-validation and its variations. This scene builds on prior explanations of cross-validation methods, particularly the advantages and computational considerations of specific approaches like leave-one-out cross-validation. The presenter (likely "person #1") appears to be transitioning into or elaborating on a specialized topic within the broader discussion.. It says " is 5 or 10. On the other hand, the V1 out cross-validation is actually...".At 00:06:38.000, The current scene is part of an educational or instructional video on statistical methods, specifically focusing on cross-validation techniques. The frame displays a computer screen with the text "a special case," continuing the discussion of specialized applications or variations of cross-validation. The audio features the presenter (likely "person #1") mentioning error rate estimation for a training sample of similar size, though the explanation is cut off mid-sentence. A sigh is heard, possibly indicating a pause or frustration. This scene builds on prior discussions of K-fold and Leave-One-Out Cross-Validation, delving into computational considerations and specific cases. The presenter appears to be transitioning into a detailed explanation of error estimation methods.. It says " actually trying to estimate the error rate for the training sample of almost the same size as what you have. So it's got low bo-".At 00:06:45.067, The current scene continues an educational video on statistical methods, specifically focusing on cross-validation techniques. The frame displays a computer screen with the text "a special case," maintaining the theme of specialized applications or variations of cross-validation. The audio features a technical explanation, likely by the presenter ("person #1"), discussing the bias-variance tradeoff in choosing the value of K for K-fold cross-validation, with a recommendation of K=5 or 10 as good choices. A sigh is heard, possibly indicating a pause or moment of reflection. This scene builds on prior discussions of cross-validation methods, emphasizing the tradeoff between bias and variance in prediction accuracy.. It says " bias, but as Rob said, high variance. So actually picking k is also a bias variance trade for prediction area. And as Rob said, k equals 5 or 10, so it tends to be a good choice. So the next...".At 00:06:59.067, The current scene is part of an educational video on statistical methods, continuing a detailed discussion of cross-validation techniques. The frame shows a computer screen displaying an image and a line of data, likely visual aids for the explanation. The audio features the presenter (likely "person #1") discussing the comparison between two-fold and 10-fold cross-validation, emphasizing the reduced variability and consistency observed with 10-fold cross-validation when partitioning data. The presenter explains that averaging the results smooths the curves, highlighting the advantages of this method over two-fold cross-validation. This scene builds on prior discussions of cross-validation methods, focusing on practical comparisons and the implications of different partitioning strategies.. It says " Next slide we've got a comparison of even out cross validation in 10 fold CV for the auto data. Remember before we saw with two fold validation, well we started with this validation into two parts, we've got a lot of variability between the 10, when we changed the sample that we, the half sample that we took. Now let's see what happens with even out cross validation, we get a curve that's, again got the minimum around the same place as we saw before, and then it's pretty flat after that. 10 fold cross validation, now again it's also showing the minimum around two, but it's, there's not the, what we're seeing here is the 10 fold cross validation as we take different partitions into 10 parts of the data, and we see there's not much variability, they're pretty consistent. In contrast to the, we divide into two parts, we've got much more variability. Those get averaged as well, those curves in the right. So, yeah, they're averaged together.".At 00:07:58.800, The current scene is part of an educational video on statistical methods, continuing a detailed discussion on cross-validation techniques. The frame shows a computer screen with the text "a special case," indicating a focus on a specific variation or scenario in cross-validation. The audio features the presenter (likely "person #1") explaining how averaging results contributes to the overall estimate of cross-validation, referencing the "overall cross-validation curve." This scene builds on prior discussions of K-fold cross-validation, leave-one-out cross-validation, and their computational tradeoffs, emphasizing the importance of averaging in reducing variability and improving accuracy.. It says " which we saw here, their average together to give us the overall estimate of cross-validation, which the overall cross-validation curve.".At 00:08:08.533, The current scene continues an educational video on statistical methods, specifically focusing on cross-validation techniques. The frame displays a computer screen showing the results of an image and a line of data, likely visual aids to support the explanation. The audio is silent except for a brief technical statement mentioning a minimum value, possibly referring to a statistical curve or metric. This scene builds on previous discussions about specialized applications of cross-validation, including K-fold and leave-one-out methods, computational considerations, and the importance of averaging results to reduce variability and improve accuracy. The presenter ("person #1") appears to be transitioning into a deeper analysis of statistical results or visual representations related to cross-validation.. It says " will look pretty much like this with its minimum around 2.".At 00:08:15.333, The current scene is part of an educational video on statistical methods, focusing on cross-validation techniques and error estimation. The frame displays a computer screen showing the results of two different simulations, accompanied by detailed audio commentary from the presenter ("person #1"). The presenter explains the comparison between true error curves and cross-validation methods (Leave-One-Out and 10-Fold), highlighting their effectiveness in approximating test error curves. Key observations include the flatness of error curves in certain regions and the proximity of minimum error values across methods. The discussion references figures from a textbook (e.g., Figures 5.6 and 2.9) and emphasizes the practical implications of model flexibility and error estimation. This scene builds on prior discussions of cross-validation techniques, providing deeper insights into statistical modeling and error analysis.. It says " Okay, this is figure 5.6 from the textbook and this is the simulated data example which was figured from figure 2.9 of the book. Just recall this is smoothing splines in three different situations. In this case the true curve, true error curve is the blue curve. Again, these three different functions that we're examining. This is mean square error for simulated data. True error curve. How do we get that? Well, it's simulated data. So we can get a very big test set and estimate the error exactly. Leave and out cross validation is the black broken line and the orange curve is 10 fold cross validation. So we can see what do we see? Well, here we see that test error curve is a little higher than the 10 fold and leave and out cross validation. The minimum is fairly close but the minimum of cross validation is around 8 whereas the true curve is minimized around 6. In this case the two cross validation methods are doing a better job of approximating the test error curve and have the minimum, well, the minimum is fairly close, not exactly on the mark. Black curve is minimized around 6 and the true error curve is minimized around 3. Although those error curves are fairly flat. So there's obviously a high variance in where the minimum should be. Right. And that doesn't really matter. That's right. It's not going to matter much if you choose a model with flexibility 2 or maybe even 10 here because the error is pretty flat in that region. And then the third example, the two cross validation curves do quite a good job of approximating the test error curve and the minimum is around 10 in each case. That's it.".At 00:10:09.200, The current scene continues an educational video on statistical methods, specifically focusing on cross-validation techniques. The frame displays an object with the text "object," though its relevance to the discussion is unclear. The audio features the presenter ("person #1") reiterating points about the bias-variance tradeoff in cross-validation, emphasizing that smaller training sets in cross-validation can lead to increased bias, while larger sets reduce bias but may increase variance. The presenter recommends K=5 or 10 as a good compromise for K-fold cross-validation. This scene builds on prior discussions of cross-validation methods, maintaining the focus on balancing bias and variance for optimal prediction accuracy.. It says " So actually I said this already, but I'll say it again that one issue with cross-validations that since the training set is not as big as the original training set, the essence of prediction will be biased up a little bit because maybe less data we're working with. And I also said, and I'll say again, that we will not cross-validation has smaller bias in this sense because the training set is almost the same size as the original set. On the other hand, it's got higher variance because the training sets that it's using are almost the same as the original set. We're only given by one observation. So K equals 5 or 10 fold is a good compromise for this bias variance.".At 00:10:51.200, The current scene is part of an educational video on statistical methods, continuing a detailed discussion on cross-validation techniques. The frame shows a calculation problem, likely a visual aid to support the explanation. The audio features the presenter ("person #1") elaborating on the process of K-fold cross-validation, emphasizing the calculation of cross-validation error and its standard error. The presenter explains the importance of using weighted averages for folds of varying sizes and highlights the need to account for variability by including standard error bands in cross-validation curves. A key point discussed is the assumption of independence in error calculations, which is not strictly valid due to overlapping training samples, though the method remains effective in practice. The presenter transitions to a mention of the bootstrap method, hinting at upcoming topics. This scene builds on prior discussions of cross-validation, reinforcing its importance in error estimation and model evaluation.. It says " trade-off. Okay, so in the, we talked about cross-validation for a quantitative response, we use mean-square error, for classification problems, that he is exactly the same. The only thing that changes is the measure of error, of course, you no longer use square error, but on this classification error. Otherwise, cross-validation process is exactly the same. Divide the data up into k parts. We train on k minus 1 parts. We record the error on the kth part, and we add things up together to get the overall cross-validation error. It looks like a weighted average in that formula. It was nk over n. Well, do you want to explain that? Because each of the folds might not be exactly the same size. So we actually compute a weight which is proportioned to this, which is the relative size of the fold, and then use a weighted average. Right, and we are lucky that the k divides, the n divides by k, exactly the weight of this one, one over k. One other thing to add, which is that since this cross-validation error is just an average, the standard error of that average also gives us a standard error of the cross-validation estimate. So we take the error rates from each of the folds. The average is the cross-validation error rate. The standard error is the standard deviation of the cross-validation estimate. So here is the formula for that. So this is a useful quantity. And when we draw a CV curve, we should always put a standard error band around the curve to get rid of the variability. So in these previous pictures, we should have had a standard error band around the curves to give us an idea of how variable they are. I say here it's a useful lesson, but not quite valid. Why is that? Dr. Heastie? Well, I wonder why. Well, the thing is we compute in the standard errors if these were independent observations, but they're not strictly independent. Error sub k overlaps with error sub j because they share some training samples. So there's some correlation between that. But we use this anyway. We use it and it's actually quite a good estimate and people have shown this mathematically. Again, a point being that is that the cross-validation separates the training part of it from the validation part. When we talk about the bootstrap method, the next part of this section, we'll see that that's not the case and that's going to cause a problem. So cross-validation explicitly separates the training set from the validation set in order to get a good idea of test error. Okay, so this again, I want to re-emphasize that cross-validation is a very important technique to understand both for quantitative response and cross-validation classification.".